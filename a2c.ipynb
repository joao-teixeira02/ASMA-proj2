{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlantis - Reinforcement Learning - A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the models with different timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/RPPO\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ALE/Atlantis-v5', render_mode=\"rgb_array\", obs_type=\"grayscale\")\n",
    "env.reset()\n",
    "\n",
    "# Initialize the model\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=0, device=\"cuda\", tensorboard_log=logdir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(locals):\n",
    "    print(f\"Episode: {locals['iteration']}\")\n",
    "    print(f\"Lives: {locals['infos'][0]['lives']}\")\n",
    "    print(f\"Reward: {locals['infos'][0]['episode']['r']}\")\n",
    "    print(f\"L: {locals['infos'][0]['episode']['l']}\")\n",
    "    print(f\"T: {locals['infos'][0]['episode']['t']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseAlgorithm\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env # type: VecEnv\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # num_timesteps = n_envs * n times env.step() was called\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = {}  # type: Dict[str, Any]\n",
    "        # self.globals = {}  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger # type: stable_baselines3.common.logger.Logger\n",
    "        # Sometimes, for event callback, it is useful\n",
    "        # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.episodes_stats = []\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            print_stats(self.locals)\n",
    "\n",
    "            stat = {\n",
    "                \"episode\": self.locals['iteration'],\n",
    "                \"lives\": self.locals['infos'][0]['lives'],\n",
    "                \"reward\": self.locals['infos'][0]['episode']['r'],\n",
    "                \"l\": self.locals['infos'][0]['episode']['l'],\n",
    "                \"t\": self.locals['infos'][0]['episode']['t']\n",
    "            }\n",
    "\n",
    "            self.episodes_stats.append(stat)\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward at iteration 1: 400.0\n",
      "Total reward at iteration 1: 700.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m iters \u001b[38;5;241m<\u001b[39m MAX_ITERS:\n\u001b[0;32m      9\u001b[0m     iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIME_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Print rewards every PRINT_EVERY timesteps\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\asma\\Lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:454\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfRecurrentPPO,\n\u001b[0;32m    447\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    453\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfRecurrentPPO:\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\asma\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\asma\\Lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:336\u001b[0m, in \u001b[0;36mRecurrentPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m approx_kl_divs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Convert discrete action from float to long\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\asma\\Lib\\site-packages\\sb3_contrib\\common\\recurrent\\buffers.py:195\u001b[0m, in \u001b[0;36mRecurrentRolloutBuffer.get\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs:\n\u001b[0;32m    194\u001b[0m     batch_inds \u001b[38;5;241m=\u001b[39m indices[start_idx : start_idx \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 195\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_change\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\asma\\Lib\\site-packages\\sb3_contrib\\common\\recurrent\\buffers.py:211\u001b[0m, in \u001b[0;36mRecurrentRolloutBuffer._get_samples\u001b[1;34m(self, batch_inds, env_change, env)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Number of sequences\u001b[39;00m\n\u001b[0;32m    210\u001b[0m n_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_start_indices)\n\u001b[1;32m--> 211\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    212\u001b[0m padded_batch_size \u001b[38;5;241m=\u001b[39m n_seq \u001b[38;5;241m*\u001b[39m max_length\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# We retrieve the lstm hidden states that will allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# to properly initialize the LSTM at the beginning of each sequence\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\asma\\Lib\\site-packages\\sb3_contrib\\common\\recurrent\\buffers.py:36\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(seq_start_indices, seq_end_indices, device, tensor, padding_value)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03mChunk sequences and pad them to have constant dimensions.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m:return: (n_seq, max_length, *tensor_shape)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Create sequences given start and end\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m seq \u001b[38;5;241m=\u001b[39m [\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(seq_start_indices, seq_end_indices)]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(seq, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39mpadding_value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "custom = CustomCallback()\n",
    "\n",
    "TIME_STEPS = 100000\n",
    "MAX_ITERS = 10\n",
    "iters = 0\n",
    "\n",
    "while iters < MAX_ITERS:\n",
    "\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIME_STEPS, reset_num_timesteps=False, tb_log_name=\"RPPO\")\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(f\"{models_dir}/{TIME_STEPS*iters}.zip\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ALE/Atlantis-v5', render_mode=\"human\", obs_type=\"grayscale\")\n",
    "env.reset()\n",
    "\n",
    "while iters < MAX_ITERS:\n",
    "\n",
    "    iters += 1\n",
    "\n",
    "    model_path = f\"{models_dir}/{TIME_STEPS*iters}.zip\"\n",
    "    model = A2C.load(model_path, env=env)\n",
    "\n",
    "    episodes = 5\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        vec_env = model.get_env()\n",
    "        obs = vec_env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            \n",
    "            # Convert action to integer if it's in array form\n",
    "            if isinstance(action, np.ndarray):\n",
    "                action = action.item()\n",
    "            \n",
    "            obs, rewards, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            env.render()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
