{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlantis - Reinforcement Learning - A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the models with different timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"models/A2C\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ALE/Atlantis-v5', render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "# Initialize the model\n",
    "model = A2C('MlpPolicy', env, verbose=1, device=\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseAlgorithm\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env # type: VecEnv\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # num_timesteps = n_envs * n times env.step() was called\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = {}  # type: Dict[str, Any]\n",
    "        # self.globals = {}  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger # type: stable_baselines3.common.logger.Logger\n",
    "        # Sometimes, for event callback, it is useful\n",
    "        # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.episodes_stats = []\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            print(f\"Episode: {self.locals['iteration']}\")\n",
    "            print(f\"Lives: {self.locals['infos'][0]['lives']}\")\n",
    "            print(f\"Reward: {self.locals['infos'][0]['episode']['r']}\")\n",
    "            print(f\"L: {self.locals['infos'][0]['episode']['l']}\")\n",
    "            print(f\"T: {self.locals['infos'][0]['episode']['t']}\")\n",
    "\n",
    "            stat = {\n",
    "                \"episode\": self.locals['iteration'],\n",
    "                \"lives\": self.locals['infos'][0]['lives'],\n",
    "                \"reward\": self.locals['infos'][0]['episode']['r'],\n",
    "                \"l\": self.locals['infos'][0]['episode']['l'],\n",
    "                \"t\": self.locals['infos'][0]['episode']['t']\n",
    "            }\n",
    "\n",
    "            self.episodes_stats.append(stat)\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | -8.82e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.0224   |\n",
      "|    value_loss         | 0.000327  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.15     |\n",
      "|    explained_variance | -2.32e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.00116  |\n",
      "|    value_loss         | 6.62e-07  |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 13900.0\n",
      "L: 1227\n",
      "T: 22.637898\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+03 |\n",
      "|    ep_rew_mean        | 1.39e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.000125 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00606 |\n",
      "|    value_loss         | 2.15e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.23e+03  |\n",
      "|    ep_rew_mean        | 1.39e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.19     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.00596  |\n",
      "|    value_loss         | 4.13e-05  |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 12500.0\n",
      "L: 1067\n",
      "T: 38.926147\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+03 |\n",
      "|    ep_rew_mean        | 1.32e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 61       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 145      |\n",
      "|    value_loss         | 3.84e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.15e+03  |\n",
      "|    ep_rew_mean        | 1.32e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.15     |\n",
      "|    explained_variance | -4.15e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.00339  |\n",
      "|    value_loss         | 1.15e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+03 |\n",
      "|    ep_rew_mean        | 1.32e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 63       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.0233  |\n",
      "|    value_loss         | 0.000315 |\n",
      "------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 14900.0\n",
      "L: 1307\n",
      "T: 58.217616\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+03  |\n",
      "|    ep_rew_mean        | 1.38e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 63       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 8.17e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00839 |\n",
      "|    value_loss         | 9.6e-05  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.2e+03   |\n",
      "|    ep_rew_mean        | 1.38e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.19     |\n",
      "|    explained_variance | -8.34e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.0152   |\n",
      "|    value_loss         | 0.000208  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+03  |\n",
      "|    ep_rew_mean        | 1.38e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 83       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 3.41e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00321 |\n",
      "|    value_loss         | 1.23e-05 |\n",
      "------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 21000.0\n",
      "L: 1585\n",
      "T: 87.719284\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.3e+03   |\n",
      "|    ep_rew_mean        | 1.56e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.04     |\n",
      "|    explained_variance | -1.47e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.0103   |\n",
      "|    value_loss         | 0.000117  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+03  |\n",
      "|    ep_rew_mean        | 1.56e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 1.35e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00601 |\n",
      "|    value_loss         | 3.58e-05 |\n",
      "------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 12800.0\n",
      "L: 1143\n",
      "T: 107.140098\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+03 |\n",
      "|    ep_rew_mean        | 1.5e+04  |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 108      |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 2.35e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00235 |\n",
      "|    value_loss         | 1.17e-05 |\n",
      "------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 5000.0\n",
      "L: 639\n",
      "T: 117.067401\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.16e+03  |\n",
      "|    ep_rew_mean        | 1.34e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.933    |\n",
      "|    explained_variance | -5.56e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 0.00181   |\n",
      "|    value_loss         | 4.64e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.16e+03  |\n",
      "|    ep_rew_mean        | 1.34e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.673    |\n",
      "|    explained_variance | 4.8e-05   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.000774 |\n",
      "|    value_loss         | 1.1e-05   |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 10600.0\n",
      "L: 947\n",
      "T: 133.371038\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.13e+03  |\n",
      "|    ep_rew_mean        | 1.3e+04   |\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.413    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.0031   |\n",
      "|    value_loss         | 9.91e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.13e+03  |\n",
      "|    ep_rew_mean        | 1.3e+04   |\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.285    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.000148 |\n",
      "|    value_loss         | 5.75e-06  |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 2900.0\n",
      "L: 637\n",
      "T: 144.676576\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07e+03  |\n",
      "|    ep_rew_mean        | 1.17e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.17     |\n",
      "|    explained_variance | -0.000151 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -3.28e-05 |\n",
      "|    value_loss         | 1.12e-06  |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 5200.0\n",
      "L: 665\n",
      "T: 156.29327\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.02e+03  |\n",
      "|    ep_rew_mean        | 1.1e+04   |\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0992   |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -0.000168 |\n",
      "|    value_loss         | 0.000114  |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 4600.0\n",
      "L: 617\n",
      "T: 166.268411\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 983      |\n",
      "|    ep_rew_mean        | 1.03e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 167      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.101   |\n",
      "|    explained_variance | 0.432    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.61e-08 |\n",
      "|    value_loss         | 1.04e-12 |\n",
      "------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 2000.0\n",
      "L: 423\n",
      "T: 173.078547\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 932       |\n",
      "|    ep_rew_mean        | 9.58e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0652   |\n",
      "|    explained_variance | 9.77e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -7.83e-06 |\n",
      "|    value_loss         | 6.62e-07  |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 1900.0\n",
      "L: 573\n",
      "T: 182.437278\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 902      |\n",
      "|    ep_rew_mean        | 8.94e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.065   |\n",
      "|    explained_variance | -0.295   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 8.96e-09 |\n",
      "|    value_loss         | 8.58e-13 |\n",
      "------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 2000.0\n",
      "L: 423\n",
      "T: 190.841774\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 866       |\n",
      "|    ep_rew_mean        | 8.41e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0555   |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -7.62e-08 |\n",
      "|    value_loss         | 9.26e-11  |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 2200.0\n",
      "L: 423\n",
      "T: 197.684005\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 834       |\n",
      "|    ep_rew_mean        | 7.96e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0935   |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -4.99e-05 |\n",
      "|    value_loss         | 1.11e-05  |\n",
      "-------------------------------------\n",
      "Episode: 0\n",
      "Lives: 0\n",
      "Reward: 2000.0\n",
      "L: 423\n",
      "T: 206.904548\n"
     ]
    }
   ],
   "source": [
    "custom = CustomCallback()\n",
    "\n",
    "TIME_STEPS = 100000\n",
    "MAX_ITERS = 10\n",
    "iters = 0\n",
    "\n",
    "while iters < MAX_ITERS:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIME_STEPS, reset_num_timesteps=False, callback=custom)\n",
    "    \n",
    "    model.save(f\"{models_dir}/A2C_Atlantis_{iters}\")\n",
    "\n",
    "# store the stats\n",
    "import json\n",
    "with open(f\"{logdir}/A2C_Atlantis.json\", 'w') as f:\n",
    "    json.dump(custom.episodes_stats, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_ITERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALE/Atlantis-v5\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m iters \u001b[38;5;241m<\u001b[39m \u001b[43mMAX_ITERS\u001b[49m:\n\u001b[0;32m      9\u001b[0m     iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTIME_STEPS\u001b[38;5;241m*\u001b[39miters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_ITERS' is not defined"
     ]
    }
   ],
   "source": [
    "iters = 0\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ALE/Atlantis-v5', render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "while iters < MAX_ITERS:\n",
    "\n",
    "    iters += 1\n",
    "\n",
    "    model_path = f\"{models_dir}/{TIME_STEPS*iters}.zip\"\n",
    "    model = A2C.load(model_path, env=env)\n",
    "\n",
    "    episodes = 5\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        vec_env = model.get_env()\n",
    "        obs = vec_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            \n",
    "            # Convert action to integer if it's in array form\n",
    "            if isinstance(action, np.ndarray):\n",
    "                action = action.item()\n",
    "            \n",
    "            obs, rewards, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            env.render()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
