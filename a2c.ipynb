{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlantis - Reinforcement Learning - A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the models with different timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/A2C\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ALE/Atlantis-v5', render_mode=\"rgb_array\", obs_type=\"grayscale\")\n",
    "env.reset()\n",
    "\n",
    "# Initialize the model\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=0, device=\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(locals):\n",
    "    print(f\"Episode: {locals['iteration']}\")\n",
    "    print(f\"Lives: {locals['infos'][0]['lives']}\")\n",
    "    print(f\"Reward: {locals['infos'][0]['episode']['r']}\")\n",
    "    print(f\"L: {locals['infos'][0]['episode']['l']}\")\n",
    "    print(f\"T: {locals['infos'][0]['episode']['t']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseAlgorithm\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env # type: VecEnv\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # num_timesteps = n_envs * n times env.step() was called\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = {}  # type: Dict[str, Any]\n",
    "        # self.globals = {}  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger # type: stable_baselines3.common.logger.Logger\n",
    "        # Sometimes, for event callback, it is useful\n",
    "        # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.episodes_stats = []\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            print_stats(self.locals)\n",
    "\n",
    "            stat = {\n",
    "                \"episode\": self.locals['iteration'],\n",
    "                \"lives\": self.locals['infos'][0]['lives'],\n",
    "                \"reward\": self.locals['infos'][0]['episode']['r'],\n",
    "                \"l\": self.locals['infos'][0]['episode']['l'],\n",
    "                \"t\": self.locals['infos'][0]['episode']['t']\n",
    "            }\n",
    "\n",
    "            self.episodes_stats.append(stat)\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = CustomCallback()\n",
    "\n",
    "TIME_STEPS = 100\n",
    "MAX_ITERS = 1000\n",
    "iters = 0\n",
    "\n",
    "while iters < MAX_ITERS:\n",
    "\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIME_STEPS, reset_num_timesteps=True)\n",
    "    \n",
    "    # Print rewards every PRINT_EVERY timesteps\n",
    "    total_reward = 0\n",
    "        \n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()[0]\n",
    "    for _ in range(TIME_STEPS):\n",
    "        \n",
    "        action, _ = model.predict(obs)\n",
    "        # Convert action to integer if it's in array form\n",
    "        if isinstance(action, np.ndarray):\n",
    "            action = action.item()\n",
    "            \n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated:\n",
    "            break\n",
    "    print(\"Total reward at iteration {}: {}\".format(iters, total_reward))\n",
    "    # Save the model\n",
    "    model.save(f\"{models_dir}/{TIME_STEPS*iters}.zip\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ALE/Atlantis-v5', render_mode=\"human\", obs_type=\"grayscale\")\n",
    "env.reset()\n",
    "\n",
    "while iters < MAX_ITERS:\n",
    "\n",
    "    iters += 1\n",
    "\n",
    "    model_path = f\"{models_dir}/{TIME_STEPS*iters}.zip\"\n",
    "    model = A2C.load(model_path, env=env)\n",
    "\n",
    "    episodes = 5\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        vec_env = model.get_env()\n",
    "        obs = vec_env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            \n",
    "            # Convert action to integer if it's in array form\n",
    "            if isinstance(action, np.ndarray):\n",
    "                action = action.item()\n",
    "            \n",
    "            obs, rewards, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            env.render()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
