{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlantis - Reinforcement Learning - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAtlantisEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomAtlantisEnv, self).__init__(env)\n",
    "        self.old_lives = 6\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Modify the reward function here\n",
    "        custom_reward = self.custom_reward_function(obs, reward, done, info)\n",
    "        \n",
    "        return obs, custom_reward, done, truncated, info\n",
    "    \n",
    "    def custom_reward_function(self, obs, reward, done, info):\n",
    "\n",
    "        if reward == 2000 or reward == 200:\n",
    "            reward += 4000\n",
    "        elif reward == 1000 or reward == 100:\n",
    "            reward += 1000\n",
    "\n",
    "        if self.old_lives > info['lives']:\n",
    "            reward += -500\n",
    "            self.old_lives = info['lives']\n",
    "\n",
    "        reward += 2**(info['episode_frame_number']/3000)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the models with different timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/CUSTOM_RECURRENT\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ALE/Atlantis-v5', render_mode=\"rgb_array\", obs_type=\"grayscale\")\n",
    "env.reset()\n",
    "\n",
    "# Wrap the environment with the custom wrapper\n",
    "env = CustomAtlantisEnv(env)\n",
    "\n",
    "# Initialize the model\n",
    "model = RecurrentPPO('MlpLstmPolicy', env, verbose=0, device=\"cuda\", tensorboard_log=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(locals):\n",
    "    print(f\"Episode: {locals['iteration']}\")\n",
    "    print(f\"Lives: {locals['infos'][0]['lives']}\")\n",
    "    print(f\"Reward: {locals['infos'][0]['episode']['r']}\")\n",
    "    print(f\"L: {locals['infos'][0]['episode']['l']}\")\n",
    "    print(f\"T: {locals['infos'][0]['episode']['t']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        env = gym.make('ALE/Atlantis-v5', render_mode=\"rgb_array\", obs_type=\"grayscale\")\n",
    "        env.reset()\n",
    "        self.test_env = env\n",
    "        self.counter = 0\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseAlgorithm\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env # type: VecEnv\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # num_timesteps = n_envs * n times env.step() was called\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = {}  # type: Dict[str, Any]\n",
    "        # self.globals = {}  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger # type: stable_baselines3.common.logger.Logger\n",
    "        # Sometimes, for event callback, it is useful\n",
    "        # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.episodes_stats = []\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            #print_stats(self.locals)\n",
    "\n",
    "            self.counter += 1\n",
    "\n",
    "            if self.counter % 10 == 0:\n",
    "\n",
    "                self.model.save(f\"{models_dir}/model_custom_recurrent\")\n",
    "\n",
    "                model_copy = model.load(f\"{models_dir}/model_custom_recurrent\", env=self.test_env)\n",
    "\n",
    "                vec_env = model_copy.get_env()\n",
    "                obs = vec_env.reset()\n",
    "                done = False\n",
    "                count_rewards = 0\n",
    "                while not done:\n",
    "                    action, _states = model_copy.predict(obs)\n",
    "                    \n",
    "                    # Convert action to integer if it's in array form\n",
    "                    if isinstance(action, np.ndarray):\n",
    "                        action = action.item()\n",
    "                    \n",
    "                    obs, rewards, terminated, truncated, info = self.test_env.step(action)\n",
    "                    done = terminated or truncated\n",
    "                    count_rewards += rewards\n",
    "\n",
    "                stat = {\n",
    "                    \"episode\": self.locals['iteration'],\n",
    "                    \"lives\": self.locals['infos'][0]['lives'],\n",
    "                    \"reward\": count_rewards,\n",
    "                    \"l\": self.locals['infos'][0]['episode']['l'],\n",
    "                    \"t\": self.locals['infos'][0]['episode']['t']\n",
    "                }\n",
    "\n",
    "                print(f\"Reward: {count_rewards}\")\n",
    "\n",
    "                self.episodes_stats.append(stat)\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = CustomCallback()\n",
    "\n",
    "TIME_STEPS = 100000\n",
    "MAX_ITERS = 10\n",
    "iters = 0\n",
    "\n",
    "while iters < MAX_ITERS:\n",
    "\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIME_STEPS, reset_num_timesteps=False, tb_log_name=\"CUSTOM_RECURRENT\", callback=custom)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(f\"{models_dir}/{TIME_STEPS*iters}.zip\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom.episodes_stats)\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming custom.episodes_stats is a dictionary\n",
    "data = custom.episodes_stats\n",
    "\n",
    "# Specify the file name where you want to save the JSON data\n",
    "file_name = \"custom_recurrent_stats.json\"\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open(file_name, \"w\") as json_file:\n",
    "    json.dump(data, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "MAX_ITERS = 10\n",
    "TIME_STEPS = 100000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ALE/Atlantis-v5', render_mode=\"human\", obs_type=\"grayscale\")\n",
    "env.reset()\n",
    "\n",
    "while iters < MAX_ITERS:\n",
    "\n",
    "    iters += 1\n",
    "\n",
    "    model_path = f\"{models_dir}/{TIME_STEPS*iters}.zip\"\n",
    "    model = RecurrentPPO.load(model_path, env=env)\n",
    "\n",
    "    print(f\"Model: {model_path}\")\n",
    "\n",
    "    episodes = 1\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        print(f\"Episode: {ep}\")\n",
    "        vec_env = model.get_env()\n",
    "        obs = vec_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            \n",
    "            # Convert action to integer if it's in array form\n",
    "            if isinstance(action, np.ndarray):\n",
    "                action = action.item()\n",
    "            \n",
    "            obs, rewards, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            env.render()\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
